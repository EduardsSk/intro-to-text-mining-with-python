{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysing Data and Visualising Results using Python\n",
    "\n",
    "* Step 5: Visualising frequency distributions\n",
    "* Revising your visualisation\n",
    "* Alternative ways of visualising\n",
    "* Critical approaches to visualisation\n",
    "* Further resources on learning more about text-mining in Python in your own time.\n",
    "\n",
    "### English Stopwords\n",
    " Before we start, we need to take a moment to think about what sort of words we are actually interested in counting. Remember our research question:\n",
    "\n",
    "> What are the top 10 words used in Homer's Iliad in English translation?\n",
    "\n",
    "We are not interested in common words in English that carry little meaning, such as 'the', 'a' and 'its'. These are called **stopwords**. There is no definitive list of stopwords, but a commonly-used list is provided by the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../nltk-data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK list of all stopwords\n",
    "nltk.download('stopwords', download_dir=Path('..', 'nltk-data'))\n",
    "\n",
    "# Import the list of stopwords we just downloaded\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the English stopwords\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "# Sort the stopwords and print the first 20\n",
    "sorted_english_stops = sorted(english_stops)\n",
    "sorted_english_stops[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Steps 4: Analysing Your Data with Frequency Analysis\n",
    "\n",
    "* Step 4: analysing your data - frequency distributions - counting words\n",
    "\n",
    "## What's Next?\n",
    "You will get the most out of this workshop if you can follow up on the learning over the next few days before you forget it all! This is particularly important when learning to code. The abstract concepts need to be reinforced little and often.\n",
    "\n",
    "Recommended next steps:\n",
    "\n",
    "* Install Python using Anaconda on your computer: [Installing Anaconda on Windows](https://www.datacamp.com/community/tutorials/installing-anaconda-windows) [Installing Anaconda on Mac](https://www.datacamp.com/community/tutorials/installing-anaconda-mac-os-x).\n",
    "* Work through this series of [Programming Historian tutorials](https://programminghistorian.org/en/lessons/working-with-text-files) to get some more practice with text files and basic text-mining techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
