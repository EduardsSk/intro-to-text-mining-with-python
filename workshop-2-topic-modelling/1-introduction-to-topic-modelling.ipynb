{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modelling with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## What is Topic Modelling?\n",
    "\n",
    "Topic modelling is a _distant reading_ technique for finding structure in large collections of text, without actually reading everything by eye. If you have hundreds or thousands of documents and want to understand roughly what your corpus contains, then topic modelling may be for you.\n",
    "\n",
    "A topic modelling programme finds the words that appear frequently together in a document and groups them together to form 'topics'. A **topic** is a mixture of words that is supposed to characterise (part of) the content of a document — its theme or underlying ideas. For example, one topic of this [Wikipedia article](https://en.wikipedia.org/wiki/Black_hole) is:\n",
    "\n",
    "* black, hole, mass, star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![First picture of a supermassive black hole, captured in 2019](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Black_hole_-_Messier_87.jpg/320px-Black_hole_-_Messier_87.jpg \"First picture of a supermassive black hole, captured in 2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too surprising, you may think. We could say the topic seems pretty accurate from our perspective. What about a document that we are less familiar with? Here is a topic of a [speech](https://er.jsc.nasa.gov/seh/ricetalk.htm) made by John F. Kennedy at Rice University in 1962:\n",
    "\n",
    "* space, new, year, man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Surveyor_3-Apollo_12.jpg/274px-Surveyor_3-Apollo_12.jpg \"Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Kennedy's famous 'we choose to go to the moon' speech. Notice that 'moon' is not in this topic; but the speech does cover the history of humankind's (\"man's\") endeavours and emphasises a forward-looking perspective (the \"new\"-ness of advancements).\n",
    "\n",
    "From these simplified examples, we can see that human intervention is still required to interpret what topics might 'mean'. Topic modelling is not magic; it is a tool that requires informed use and careful review, just like any other.\n",
    "\n",
    "### So... Why Do Topic Modelling?\n",
    "In the humanities, topic modelling may be used to support different approaches to large text corpora, such as:\n",
    "\n",
    "* Survey a collection that is too big to read closely e.g. [Computational Historiography: Data Mining in a Century of Classics Journals](http://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf) (PDF)\n",
    "* Look at thematic trends over time in an archive e.g. [Topic Modeling Martha Ballard's Diary](http://www.cameronblevins.org/posts/topic-modeling-martha-ballards-diary/)\n",
    "* Create metadata for an archive to improve accessibility e.g. [Topic modelling for the valorisation of digitised archives of the European Commission](https://ieeexplore.ieee.org/abstract/document/7840981)\n",
    "* Understand current trends in social media relevant to your discipline e.g. [Mining the Open Web with ‘Looted Heritage’](https://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/)\n",
    "\n",
    "### Alternatives to Topic Modelling in Python\n",
    "If you are looking to explore the topics of a few documents in a casual way, you can use the online digital texts environment [Voyant](), which allows you to upload or copy-and-paste texts and explore a corpus with a number of graphical tools, including topics.\n",
    "\n",
    "For serious research, a well-known tool for topic modelling is called [MALLET](http://mallet.cs.umass.edu/topics.php), which is a programme (written in Java) that you download to your computer. You have to type commands to use MALLET, but it has otherwise done a great deal for you. [Getting Started with Topic Modeling and MALLET](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet) from Programming Historian gives a step-by-step tutorial on MALLET.\n",
    "\n",
    "There is a graphical interface for MALLET called [Topic Modeling Tool](https://github.com/senderle/topic-modeling-tool) that is a bit easier to use. The [Quickstart Guide](https://senderle.github.io/topic-modeling-tool/documentation/2017/01/06/quickstart.html) will get you up and running.\n",
    "\n",
    "If you are looking to use R rather than Python, then `tidytext` is a popular NLP library that will help you work with the `topicmodels` package. The book _Text Mining with R_ devotes [chapter 6](https://www.tidytextmining.com/topicmodeling.html) to this.\n",
    "\n",
    "---\n",
    "**With the alternatives out of the way, let's see how we can do topic modelling in Python!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## How to Join In with Coding\n",
    "\n",
    "* **Edit** any cell and try changing the code, or delete it and write your own.\n",
    "\n",
    "* Before running a cell, try to **guess** what the output will be by thinking through what will happen.\n",
    "\n",
    "* If you encounter an **error**, realise this is normal. Errors happen all the time and by reading the error message you will learn something new.\n",
    "\n",
    "* Remember: you cannot break the notebook or your computer, so **don't be afraid to experiment**.\n",
    "\n",
    "**Let's get coding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Recap of Python Basics\n",
    "Welcome back! Let's recap the Python that we learnt last time. Any questions?\n",
    "### Strings\n",
    "Create a _string_ and store it with a _name_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon formed 4.51 billion years ago.'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence = 'The Moon formed 4.51 billion years ago.'\n",
    "my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Slice_ a string. Remember that indexing in Python starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon formed 4.51'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a string with string methods. Important: the original string `my_sentence` is unchanged. Instead, a string method _returns_ a new string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tHE mOON FORMED 4.51 BILLION YEARS AGO.'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.swapcase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a string with string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.islower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a string to see if it contains another string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'f' in my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a _list_ of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Moon formed 4.51 billion years ago',\n",
       " \"The Moon is Earth's only permanent natural satellite\",\n",
       " 'The Moon was first reached in September 1959']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['The Moon formed 4.51 billion years ago',\n",
    "           \"The Moon is Earth's only permanent natural satellite\",\n",
    "          'The Moon was first reached in September 1959']\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon was first reached in September 1959'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformed list of strings with a _list comprehension_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"THE MOON IS EARTH'S ONLY PERMANENT NATURAL SATELLITE\"]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [string.upper() for string in my_list if 'Earth' in string]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "`import` a _module_ and use it. A module is simply code 'written by someone else' in another file (or files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE FIRST MEN IN THE MOON\\r\\n\\r\\nby H.G. Wells\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nChapter 1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMr. Bedford Meets Mr. Cavor at Lympne\\r\\n\\r\\nAs I sit down to write here amidst the shadows of vine-leaves under the\\r\\nblue sky of southern Italy, it com'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/1/1013/1013.txt')\n",
    "text = response.text\n",
    "text[681:900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import` [Natural Language Tool Kit](http://www.nltk.org/) (NLTK) to help with natural language processing (NLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'FIRST',\n",
       " 'MEN',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'MOON',\n",
       " 'by',\n",
       " 'H.G',\n",
       " '.',\n",
       " 'Wells',\n",
       " 'Chapter',\n",
       " '1',\n",
       " 'Mr.',\n",
       " 'Bedford',\n",
       " 'Meets',\n",
       " 'Mr.',\n",
       " 'Cavor',\n",
       " 'at',\n",
       " 'Lympne',\n",
       " 'As']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens[126:146]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "_Call_ a _function_ with _arguments_. For example, here the function `most_common()` takes a single argument `10`, to give us the ten most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4612),\n",
       " ('.', 3851),\n",
       " ('the', 3639),\n",
       " ('and', 2538),\n",
       " ('of', 2483),\n",
       " ('I', 2190),\n",
       " ('a', 1809),\n",
       " ('to', 1658),\n",
       " (\"''\", 1214),\n",
       " ('``', 1160)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freqdist = FreqDist(tokens)\n",
    "freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## More About Tokenising and Normalisation\n",
    "\n",
    "In the last workshop, in notebook `workshop-1-basics/2-collecting-and-preparing.ipynb`, we cleaned and prepared the text _The Iliad of Homer_ (translated by Alexander Pope (1899)) by:\n",
    "* Tokenising the text into individual words.\n",
    "* Normalising the text:\n",
    " * into lowercase,\n",
    " * removing punctuation,\n",
    " * removing non-words (empty strings, numerals, etc.),\n",
    " * removing stopwords.\n",
    "\n",
    "One form of normalisation we didn't do last time is making sure that different _inflections_ of the same word are counted together. In English, words are modified to express quantity, tense, etc. (i.e. _declension_ and _conjugation_ for those who remember their language lessons!).\n",
    "\n",
    "For example, 'fish', 'fishes', 'fishy' and 'fishing' are all formed from the root 'fish'. Last workshop, all these words would have been counted as different words, which may or may not be desirable.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "There are two main ways to normalise for inflection:\n",
    "\n",
    "* **Stemming** - reducing a word to a stem by removing endings (a **stem** may not be an actual word).\n",
    "* **Lemmatization** - reducing a word to its meaningful base form using its context (a **lemma** is typically a proper word in the language).\n",
    "\n",
    "To do this we can use several facilities provided by NLTK. There are many different ways to stem and lemmatize words, but we will compare the results of the [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) and [WordNet](https://wordnet.princeton.edu/) lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All about us on the sunlit slopes frothed and swayed the darting shrubs'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_wells = text[118017:118088]\n",
    "hg_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'about',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'froth',\n",
       " 'and',\n",
       " 'sway',\n",
       " 'the',\n",
       " 'dart',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(hg_wells)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(token) for token in tokens]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mary/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'frothed',\n",
       " 'and',\n",
       " 'swayed',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the results? Perhaps surprisingly, the lemmatizer seems to have performed more poorly than the stemmer since `frothed` and `darting` have not been reduced to `froth` and `dart`.\n",
    "\n",
    "The different rules used to stem and lemmatize words are called _algorithms_ and they can result in different stems and lemmas. If the precise details of this are important to your research, you should compare the results of the various algorithms. Stemmers and lemmatizers are also available in many languages, not just English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Improving Lemmatization with Part-of-Speech Tagging\n",
    "\n",
    "To improve the lemmatizer's performance we can tell it which _part of speech_ each word is, which is known as **part-of-speech tagging (POS tagging)**. A part of speech is the role a word plays in the sentence, e.g. verb, noun, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the POS tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('All', 'DT'),\n",
       " ('about', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sunlit', 'NN'),\n",
       " ('slopes', 'VBZ'),\n",
       " ('frothed', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('swayed', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('darting', 'NN'),\n",
       " ('shrubs', 'NN')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the POS tags for each token\n",
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags that NLTK generates are from the [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/MBSP-tags). For example, now we know that `frothed` is a 'verb, past participle' (VBN).\n",
    "\n",
    "Unfortunately, the NLTK lemmatizer accepts WordNet tags (`ADJ, ADV, NOUN, VERB = 'a', 'r', 'n', 'v'`) instead! In theory, at least, if we pass the tagging information to the lemmatizer, the results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slop',\n",
       " 'froth',\n",
       " 'and',\n",
       " 'sway',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping of tokens to WordNet POS tags\n",
    "tags = [('All', 'n'),\n",
    " ('about', 'n'),\n",
    " ('us', 'n'),\n",
    " ('on', 'n'),\n",
    " ('the', 'n'),\n",
    " ('sunlit', 'a'),\n",
    " ('slopes', 'v'),\n",
    " ('frothed', 'v'),\n",
    " ('and', 'n'),\n",
    " ('swayed', 'v'),\n",
    " ('the', 'n'),\n",
    " ('darting', 'a'),\n",
    " ('shrubs', 'n')]\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(*tag) for tag in tags]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `frothing` has been reduced to `froth`. In practice, however, we may wish to [experiment](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) with other lemmatizers to get the best results. The [SpaCy](https://spacy.io/) Python library has an excellent alternative lemmatizer, for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Beyond NLTK to SpaCy\n",
    "\n",
    "NLTK was the first open-source Python library for Natural Language Processing (NLP), originally released in 2001, and it is still a valuable tool for teaching and research. Much of the literature uses NLTK code in its examples, which is why I chose to write this course using NLTK. As you may deduce from the parts-of-speech tagging example (above), NLTK does have its limitations though.\n",
    "\n",
    "In many ways NLTK has been overtaken in efficiency and ease of use by other, more modern libraries, such as [SpaCy](https://spacy.io/). SpaCy is designed to use less computer memory and split workloads across multiple processor cores (or even computers) so that it can handle very large corpora easily. It also has excellent documentation. If you are serious about text-mining with Python for a large research dataset, I recommend that you try SpaCy. If you have understood the text-mining principles we have covered with NLTK, you will have no trouble using SpaCy as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Gensim Python Library for Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/) is an open-source library that specialises in topic modelling. It is powerful, easy to use and is designed to work with very large corpora. (Another Python library, [scikit-learn](https://scikit-learn.org), also has topic modelling, but we won't cover that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Example Corpus: US Presidential Inaugural Addresses\n",
    "\n",
    "First, we are going to load a corpus of speeches `nltk.corpus.inaugural` that comes packaged into NLTK. This is the C-Span Inaugural Address Corpus (public domain) that contains the inaugural address of every US president from 1789–2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /home/mary/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "inaugural = nltk.corpus.inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what is inside, we can list the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = inaugural.fileids()\n",
    "files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And examine the first few words of each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'am', 'again', ...]\n",
      "['When', 'it', 'was', 'first', 'perceived', ',', 'in', ...]\n",
      "['Friends', 'and', 'Fellow', 'Citizens', ':', 'Called', ...]\n",
      "['Proceeding', ',', 'fellow', 'citizens', ',', 'to', ...]\n",
      "['Unwilling', 'to', 'depart', 'from', 'examples', 'of', ...]\n",
      "['About', 'to', 'add', 'the', 'solemnity', 'of', 'an', ...]\n",
      "['I', 'should', 'be', 'destitute', 'of', 'feeling', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'shall', 'not', ...]\n",
      "['In', 'compliance', 'with', 'an', 'usage', 'coeval', ...]\n"
     ]
    }
   ],
   "source": [
    "for file in files[0:10]:\n",
    "    print(inaugural.words(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Corpora for Learning and Practicing Text-Mining\n",
    "It is difficult to source pre-prepared corpora for learning and practicing text-mining. The documents must be good quality, easily available and distributed with a license that allows text-mining. NLTK comes with a number of corpora you can download from [`nltk_data`](http://www.nltk.org/nltk_data/) but these are quite old and limited in scope. It's worth searching around for [lists of corpora](https://nlpforhackers.io/corpora/) but bear in mind you must determine the true source and licensing of any corpus for yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text in Gensim\n",
    "Before we can start to do topic modelling we must — of course! — clean and prepare the text by tokenising, removing stopwords, stemming, and so on. We could do this with NLTK, as we have learnt, but Gensim can do that for us too.\n",
    "\n",
    "The defaults of `preprocess_string()` and `preprocess_documents()` use the following _filters_:\n",
    "\n",
    "* Strip any HTML or XML tags\n",
    "* Replace punctuation characters with spaces\n",
    "* Remove repeating whitespace characters and turn tabs and line breaks into spaces\n",
    "* Remove digits\n",
    "* Remove stopwords\n",
    "* Remove words with length less than 3 characters\n",
    "* Lowercase\n",
    "* Stem the words using a Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'senat',\n",
       " 'hous',\n",
       " 'repres',\n",
       " 'vicissitud',\n",
       " 'incid',\n",
       " 'life',\n",
       " 'event',\n",
       " 'fill']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "# Pre-process the first file in the corpus as an example\n",
    "washington = files[0]\n",
    "text = inaugural.raw(washington)\n",
    "tokens = preprocess_string(text)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter Stemmer that comes with Gensim does not give us real words; this will make our topics less readable. In order to lemmatize the words instead, we have to specify a _list of filters_ that we want `preprocess_string()` to apply.\n",
    "\n",
    "Before that we will import an alternative lemmatizer from the [SpaCy](https://spacy.io/) library, as it is a better by default than the NLTK one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/mary/PycharmProjects/intro-to-text-mining-with-python/venv/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/mary/PycharmProjects/intro-to-text-mining-with-python/venv/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/mary/PycharmProjects/intro-to-text-mining-with-python/venv/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sway'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.lemmatizer import LOOKUP\n",
    "lemmatize = Lemmatizer(lookup=LOOKUP).lookup\n",
    "lemmatize('swayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(👆👆👆 All you need to understand here is that we using SpaCy's lemmatizer rather than NLTK's. If you don't understand the code, you can skip over it and continue.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a list of filters, which are in fact the same as defaults, except with the string method `lower()` and without the Gensim stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'stand',\n",
       " 'today',\n",
       " 'humble',\n",
       " 'task',\n",
       " 'grateful',\n",
       " 'trust',\n",
       " 'bestow',\n",
       " 'mindful']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = [strip_tags, \n",
    "           strip_punctuation, \n",
    "           strip_multiple_whitespaces, \n",
    "           strip_numeric, \n",
    "           remove_stopwords, \n",
    "           strip_short,\n",
    "           str.lower]\n",
    "\n",
    "# Pre-process the tokens with the filters\n",
    "tokens = preprocess_string(text, filters=filters)\n",
    "\n",
    "# Lemmatize the filtered tokens with SpaCy's lemmatizer\n",
    "lemmas = [lemmatize(token) for token in tokens]\n",
    "lemmas[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## More Python Essentials\n",
    "Before we go on to the next notebook `2-topic-modelling-fundamentals.ipynb` we need to a cover a few more Python essentials.\n",
    "\n",
    "### Looping with `for` loops\n",
    "We have already seen **`for` loops** in passing when we create new lists using list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock', 'paper', 'scissors']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = ['rock', 'paper', 'scissors']\n",
    "new_list = [move for move in game]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special form of loop for comprehensions — but it essentially works the same as the normal kind.\n",
    "\n",
    "The normal kind of `for` loop looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "paper\n",
      "scissors\n"
     ]
    }
   ],
   "source": [
    "for move in game:\n",
    "    print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `for` loop goes over every item in a list in turn — and runs whatever code is inside its _block_. It makes sure that every item is visited, and then it stops when it gets to the end. We call this **iteration**; the loop _iterates_ over the list. (Loops also work for many other types of things, like strings.)\n",
    "\n",
    "Note that a _block_ of code in Python is indicated by _indenting_ the code by several spaces (typically four spaces).\n",
    "\n",
    "You also get loops inside other loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "True\n",
      "False\n",
      "True\n",
      "straw\n",
      "twigs\n",
      "bricks\n"
     ]
    }
   ],
   "source": [
    "lists = [\n",
    "    [0, 1, 2],\n",
    "    [True, False, True],\n",
    "    ['straw', 'twigs', 'bricks']\n",
    "]\n",
    "\n",
    "for lst in lists:\n",
    "    for item in lst:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "Dictionaries are a form of _mapping_. They map **keys** to **values**. You can think of it like the index at the back of a book, where the key is a word and its value is the page number where you can find that word in the book. To find the page number of a word, you look through the index and find the word you want (the key) and then look at the number (the value).\n",
    "\n",
    "```\n",
    "agriculture, 228 \n",
    "air freight, 46 \n",
    "airplane food, 19 \n",
    "alcohol, 165 \n",
    "alfalfa, 242 \n",
    "```\n",
    "\n",
    "_etc._\n",
    "\n",
    "\n",
    "The Python dictionary is called a `dict` and it can hold (almost) any type of key and value: strings, numbers, Booleans (`True`, `False`) and more.\n",
    "\n",
    "To create a new `dict` we use curly braces `{}` and inside put each key and value separated by a colon `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 228,\n",
       " 'air freight': 46,\n",
       " 'airplane food': 19,\n",
       " 'alcohol': 165,\n",
       " 'alfalfa': 242}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    'agriculture': 228,\n",
    "    'air freight': 46,\n",
    "    'airplane food': 19,\n",
    "    'alcohol': 165,\n",
    "    'alfalfa': 242\n",
    "}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can find the page number (value) of any of these words (keys) by putting the key in square brackets `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['agriculture']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a new key-value pair to the dictionary we can use the key in square brackets `[]` and assign the new value to it with the assignment operator `=`. In this example, the new key is 'allergies' and the new value is '210'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 228,\n",
       " 'air freight': 46,\n",
       " 'airplane food': 19,\n",
       " 'alcohol': 165,\n",
       " 'alfalfa': 242,\n",
       " 'allergies': 210}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['allergies'] = 210\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples\n",
    "A tuple is a bit like a list, except unlike a list, tuples cannot be changed. You cannot add or remove items from a tuple once you have created it. Tuples are known as _immutable_.\n",
    "\n",
    "NB: Tuple is often pronounced 'toople' if you are from the UK, or 'tupple' if you are from the US, but it doesn't really matter.\n",
    "\n",
    "To create a new tuple we use parentheses `()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5.0, 'ten-thousand')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = (1, 5.0, 'ten-thousand')\n",
    "my_tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a list you can _slice_ a tuple, to access its items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ten-thousand'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But unlike a list, you cannot assign a new value to any of its items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-213ffe8706e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rainbows and unicorns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "my_tuple[2] = 'rainbows and unicorns'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files and Writing to File\n",
    "Last workshop I glossed over how we save to text files read them back in again. I offered this guide [Reading and Writing Files in Python](https://realpython.com/read-write-files-python/#opening-and-closing-a-file-in-python), which is an excellent in-depth look that I recommend.\n",
    "\n",
    "In brief, in order to open files we use the `open()` function and the keyword `with`.\n",
    "\n",
    "For reading:\n",
    "\n",
    "`with open(file, 'r') as reader:`\n",
    "\n",
    "For writing:\n",
    "\n",
    "`with open(file, 'w') as writer:`\n",
    "\n",
    "Then whatever you put inside the code block will run with the file open and ready. Once your code has finished running the file is safely closed.\n",
    "\n",
    "We can create and then write a text file with the `write()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blackhole.txt', 'w') as writer:\n",
    "    writer.write('At the center of a black hole lies a singularity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now go to the Jupyter notebook folder `workshop-2-topic-modelling`, open the newly created text file `blackhole.txt` and inspect its contents!\n",
    "\n",
    "We can read this file back in to a string with the `read()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At the center of a black hole lies a singularity.'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('blackhole.txt', 'r') as reader:\n",
    "    sentence = reader.read()\n",
    "    \n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write line by line (instead of the whole file at once) use `writelines()`, and likewise, to read one line at a time use `readlines()`. For all the details, see the tutorial linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Pre-Processing the Corpus and Saving to File\n",
    "We can now put everything we have learnt together to process our entire corpus of speeches, and save the clean lemma tokens to text files, ready to be loaded in the next notebook `2-topic-modelling-fundamentals`.\n",
    "\n",
    "Let's step through this code now:\n",
    "\n",
    "1. Create a location for the `data/inaugural` folder where we want to save the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "location = Path('data', 'inaugural-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loop over all the files in turn, using the Gensim `preprocess_string` function to prepare them, and save them as individual files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1789-Washington.txt\n",
      "Processing file: 1793-Washington.txt\n",
      "Processing file: 1797-Adams.txt\n",
      "Processing file: 1801-Jefferson.txt\n",
      "Processing file: 1805-Jefferson.txt\n",
      "Processing file: 1809-Madison.txt\n",
      "Processing file: 1813-Madison.txt\n",
      "Processing file: 1817-Monroe.txt\n",
      "Processing file: 1821-Monroe.txt\n",
      "Processing file: 1825-Adams.txt\n",
      "Processing file: 1829-Jackson.txt\n",
      "Processing file: 1833-Jackson.txt\n",
      "Processing file: 1837-VanBuren.txt\n",
      "Processing file: 1841-Harrison.txt\n",
      "Processing file: 1845-Polk.txt\n",
      "Processing file: 1849-Taylor.txt\n",
      "Processing file: 1853-Pierce.txt\n",
      "Processing file: 1857-Buchanan.txt\n",
      "Processing file: 1861-Lincoln.txt\n",
      "Processing file: 1865-Lincoln.txt\n",
      "Processing file: 1869-Grant.txt\n",
      "Processing file: 1873-Grant.txt\n",
      "Processing file: 1877-Hayes.txt\n",
      "Processing file: 1881-Garfield.txt\n",
      "Processing file: 1885-Cleveland.txt\n",
      "Processing file: 1889-Harrison.txt\n",
      "Processing file: 1893-Cleveland.txt\n",
      "Processing file: 1897-McKinley.txt\n",
      "Processing file: 1901-McKinley.txt\n",
      "Processing file: 1905-Roosevelt.txt\n",
      "Processing file: 1909-Taft.txt\n",
      "Processing file: 1913-Wilson.txt\n",
      "Processing file: 1917-Wilson.txt\n",
      "Processing file: 1921-Harding.txt\n",
      "Processing file: 1925-Coolidge.txt\n",
      "Processing file: 1929-Hoover.txt\n",
      "Processing file: 1933-Roosevelt.txt\n",
      "Processing file: 1937-Roosevelt.txt\n",
      "Processing file: 1941-Roosevelt.txt\n",
      "Processing file: 1945-Roosevelt.txt\n",
      "Processing file: 1949-Truman.txt\n",
      "Processing file: 1953-Eisenhower.txt\n",
      "Processing file: 1957-Eisenhower.txt\n",
      "Processing file: 1961-Kennedy.txt\n",
      "Processing file: 1965-Johnson.txt\n",
      "Processing file: 1969-Nixon.txt\n",
      "Processing file: 1973-Nixon.txt\n",
      "Processing file: 1977-Carter.txt\n",
      "Processing file: 1981-Reagan.txt\n",
      "Processing file: 1985-Reagan.txt\n",
      "Processing file: 1989-Bush.txt\n",
      "Processing file: 1993-Clinton.txt\n",
      "Processing file: 1997-Clinton.txt\n",
      "Processing file: 2001-Bush.txt\n",
      "Processing file: 2005-Bush.txt\n",
      "Processing file: 2009-Obama.txt\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    \n",
    "    print(f'Processing file: {file}')\n",
    "    \n",
    "    text = inaugural.raw(file)\n",
    "    tokens = preprocess_string(text, filters=filters)\n",
    "    lemmas = [lemmatize(token) for token in tokens]\n",
    "    \n",
    "    with open(location / file, 'w') as writer:\n",
    "        writer.write(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feel free to inspect these files now in the folder `data/inaugural-test`. If for some reason you have changed the code and it's not worked properly, don't worry! I've created a proper set to use in `data/inaugural`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook we have covered:\n",
    "\n",
    "* Recap of Python basics from last workshop:\n",
    " * Strings and lists\n",
    " * Imports\n",
    " * Functions\n",
    "* Stemming and lemmatization\n",
    "* Gensim Python library for topic modelling\n",
    "* More Python essentials:\n",
    " * Loops\n",
    " * Dictionaries and tuples\n",
    " * Reading and writing files\n",
    "\n",
    "👌👌👌\n",
    "\n",
    "In the next notebook `2-topic-modelling-fundamentals` we will walk through a full example of topic modelling using Gensim and the speeches we have prepared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
