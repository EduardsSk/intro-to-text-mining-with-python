{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modelling\n",
    "\n",
    "---\n",
    "---\n",
    "## Recap of Python Basics\n",
    "Welcome back! Before we get started, let's recap the Python that we learnt last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What exactly is topic modelling?\n",
    "Topic modelling is an **unsupervised** **classification** technique. \n",
    "\n",
    "**Natural language processing**\n",
    "... \n",
    "\n",
    "**Model**\n",
    "... \n",
    "\n",
    "**Supervised and unsupervised techniques**\n",
    "... \n",
    "\n",
    "**Classification**\n",
    "... \n",
    "\n",
    "\n",
    "### Alternatives to topic modelling in Python\n",
    "If you are just looking to explore the topics of a few documents in a casual way, you can use the online digital texts environment [Voyant](), which allows you to upload or copy-and-paste texts and explore a corpus with a number of graphical tools, including topics.\n",
    "\n",
    "For serious research, a well-known tool for topic modelling is called [MALLET](http://mallet.cs.umass.edu/topics.php), which is a programme (written in Java) that you download to your computer. You have to type commands to use MALLET, but it has otherwise done a great deal for you. [Getting Started with Topic Modeling and MALLET](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet) from Programming Historian gives a step-by-step tutorial on MALLET.\n",
    "\n",
    "There is a graphical interface for MALLET called [Topic Modeling Tool](https://github.com/senderle/topic-modeling-tool) that is a bit easier to use. The [Quickstart Guide](https://senderle.github.io/topic-modeling-tool/documentation/2017/01/06/quickstart.html) will get you up and running.\n",
    "\n",
    "If you are looking to use R rather than Python, then `tidytext` is a popular NLP library that will help you work with the `topicmodels` package. The book _Text Mining with R_ devotes [chapter 6](https://www.tidytextmining.com/topicmodeling.html) to this.\n",
    "\n",
    "With the alternatives out of the way, let's see how we can do topic modelling in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Python\n",
    "As necessary as we go along. Maybe some clarification and extension of last time's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked example\n",
    "* Corpus of more than one text\n",
    "* Remember cleaning from last time (may not actually do this in notebooks, but rather just remember)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "A bag-of-words (BoW) corpus is a _vocabulary_ of the known words in the corpus together with some _measure_ of how often they occur. The measurement may be:\n",
    "* binary (presence or absence)\n",
    "* count (how many times the word occurs)\n",
    "* frequency (count divided by the total number of words).  \n",
    "\n",
    "This combination of vocabulary and measurement is called a **document vector**.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Here is a simplified example to demonstrate the principles of creating a vector from a document.\n",
    "\n",
    "Document (20 words):\n",
    "\n",
    ">'No room to poise the lance or bend the bow;\n",
    "> But hand to hand, and man to man, they grow:'\n",
    " \n",
    " (from _The Iliad of Homer_, translated by Alexander Pope (1899)) \n",
    "  \n",
    "Vocabulary of unique words (15 words):\n",
    "\n",
    "* no\n",
    "* room\n",
    "* to\n",
    "* poise\n",
    "* the\n",
    "* lance\n",
    "* or\n",
    "* bend\n",
    "* bow\n",
    "* but\n",
    "* hand\n",
    "* and\n",
    "* man\n",
    "* they\n",
    "* grow\n",
    "\n",
    "Count measurements (how many times each word appears in the document):\n",
    "\n",
    "* no = 1\n",
    "* room = 1\n",
    "* to = 3\n",
    "* poise = 1\n",
    "* the = 2\n",
    "* lance = 1\n",
    "* or = 1\n",
    "* bend = 1\n",
    "* bow = 1\n",
    "* but = 1\n",
    "* hand = 2\n",
    "* and = 1\n",
    "* man = 2\n",
    "* they = 1\n",
    "* grow = 1\n",
    "\n",
    "If we treat this vocabulary as a list with a fixed order, we can just extract the counts into a list. This is the document vector.\n",
    "\n",
    "`[1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1]`\n",
    "\n",
    "In order to compare other documents with this one for similarity, we could generate a document vector with the same vocabulary list for each document, or expand the vocabulary list to cover all the words in all the documents we are interested in.\n",
    "\n",
    "#### The 'bag' in bag of wordsprocessing\n",
    "\n",
    "In this most basic BoW model all order and location of the words is discarded. For example, it does not matter if the words 'red' and 'nose' are adjacent ('red nose'), or at the beginning or end of a sentence; BoW just treats the words individually. It is like a 'bag' of Scrabble™ tiles, where each tile is a word, all rattling around together in no particular order.\n",
    "\n",
    "It is possible to create a BoW corpus that uses two or more adjacent words, and potentially . For example, if you measure all pairs of words in our example document (above) you might end up with a vocabulary that looks like this:\n",
    "\n",
    "* no room\n",
    "* room to\n",
    "* to poise\n",
    "* poise the\n",
    "* the lance\n",
    "* lance or\n",
    "* or bend\n",
    "* bend the\n",
    "* the bow\n",
    "* bow but\n",
    "* but hand\n",
    "* hand to\n",
    "* to hand\n",
    "* hand and\n",
    "* and man\n",
    "* man to\n",
    "* to man\n",
    "* man they\n",
    "* they grow\n",
    "\n",
    "#### n-grams\n",
    "\n",
    "Two adjacent words together like this is known as an **bigram**. The case before where we took just one word is called a **unigram**. Three words is a **trigram** and so on. These are all special cases of **n-gram**, where _n_ is some number of words.\n",
    "\n",
    "#### Vocabulary choice\n",
    "\n",
    "As you may have suspected by now, the size and nature of the vocabulary you choose is vitally important. A large vocabulary will take more computational power and memory to analyse. A vocabulary with many rare words (so the count for these words is 0) creates what is called a _sparse_ vector, which has less useful information in it. Likewise, very common but largely meaningless words are often wasteful to include, for example, we would probably want to exclude a list of **stopwords**.\n",
    "\n",
    "#### Term Frequency–Inverse Document Frequency (TF-IDF)\n",
    "If you measure word frequency, highly frequent words come to dominate your results and yet they may not be as meaningful or interesting as rarer words. For example, if you are looking at articles about the history of the Moon landings, even if you have removed all the stopwords, you may well find that the words 'lunar', 'moon', 'landing', 'orbit', and 'earth' predominate. Subtle differences in topic between documents may be lost.\n",
    "\n",
    "One way to deal with this is to use a _weighting factor_ called **TF-IDF**. A value is calculated for each word that reflects:\n",
    "* Term frequency (TF) - the number of times the word appears in the document\n",
    "* Document frequency (DF) - the number of documents in the corpus that contain the word\n",
    "\n",
    "For example, if a very uncommon word is present in two documents, this word is weighted more highly than a word that is present in all documents in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fba8c6f7748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mary/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['domain', 'gradient', 'detection', 'architecture', 'analog', 'motion', 'sensor']\n",
      "['phase', 'noise', 'oscillator', 'implantable', 'biomedical', 'application']\n",
      "['protocol', 'level', 'performance', 'analysis', 'collision', 'protocol', 'system']\n",
      "['optimization', 'method', 'joint', 'allocation', 'modulation', 'scheme', 'coding', 'rates', 'resource', 'block', 'power', 'organize', 'network']\n",
      "['similarity', 'estimation', 'using', 'locality', 'sensitive', 'hash']\n",
      "['multi', 'hysteresis', 'application', 'multi', 'scroll', 'chaotic', 'oscillator']\n",
      "['normal', 'measurement', 'visual', 'motion', 'sensor']\n",
      "['social', 'network', 'extraction', 'conference', 'participant']\n",
      "['session', 'base', 'overload', 'control', 'aware', 'server']\n",
      "['stochastic', 'learning', 'algorithm', 'application', 'contextual', 'advertising']\n",
      "['munica', 'advance', 'social', 'network', 'device', 'greeting', 'cards']\n",
      "['breath', 'energy']\n",
      "['modelling', 'analysis', 'multicell', 'converter', 'using', 'discrete', 'model']\n",
      "['wireless', 'network', 'coding', 'deciding', 'switch']\n",
      "['efficient', 'management', 'multiversion', 'document', 'object', 'reference']\n",
      "['model', 'efficient', 'flexible', 'image', 'computing']\n",
      "['experiment', 'persian', 'compression']\n",
      "['watermarking', 'conjugate', 'order', 'dither', 'block', 'truncation', 'coding', 'image']\n",
      "['revenue', 'maximize', 'pricing', 'capacity', 'expansion', 'user', 'regime']\n",
      "['dissemination', 'bounds', 'people', 'centric', 'system']\n",
      "['milgram', 'route', 'social', 'network']\n",
      "['poikilo', 'evaluate', 'result', 'diversification', 'model', 'algorithm']\n",
      "['program', 'analysis', 'conversion', 'navigation', 'specification', 'database', 'interface']\n",
      "['lossless', 'video', 'compression', 'residual', 'image', 'prediction', 'coding']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "dataset = os.path.join('data', 'dataset.csv')\n",
    "\n",
    "import random\n",
    "text_data = []\n",
    "with open(dataset) as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fba51b0cc18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1)],\n",
       " [(19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1)],\n",
       " [(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1)],\n",
       " [(7, 1), (11, 1), (38, 1), (39, 1), (40, 2), (41, 1)],\n",
       " [(5, 1), (6, 1), (42, 1), (43, 1), (44, 1)],\n",
       " [(25, 1), (45, 1), (46, 1), (47, 1), (48, 1)],\n",
       " [(49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)],\n",
       " [(7, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1)],\n",
       " [(25, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1)],\n",
       " [(65, 1), (66, 1)],\n",
       " [(13, 1), (37, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1)],\n",
       " [(21, 1), (25, 1), (72, 1), (73, 1), (74, 1)],\n",
       " [(75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1)],\n",
       " [(69, 1), (76, 1), (81, 1), (82, 1), (83, 1)],\n",
       " [(84, 1), (85, 1), (86, 1)],\n",
       " [(20, 1), (21, 1), (83, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1)],\n",
       " [(92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1)],\n",
       " [(18, 1), (99, 1), (100, 1), (101, 1), (102, 1)],\n",
       " [(25, 1), (48, 1), (103, 1), (104, 1)],\n",
       " [(56, 1), (69, 1), (105, 1), (106, 1), (107, 1), (108, 1)],\n",
       " [(13, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1)],\n",
       " [(21, 1), (83, 1), (84, 1), (115, 1), (116, 1), (117, 1), (118, 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.026*\"application\" + 0.026*\"oscillator\" + 0.026*\"motion\" + 0.026*\"sensor\"')\n",
      "(1, '0.042*\"model\" + 0.042*\"algorithm\" + 0.042*\"compression\" + 0.023*\"image\"')\n",
      "(2, '0.050*\"network\" + 0.035*\"coding\" + 0.035*\"social\" + 0.034*\"block\"')\n",
      "(3, '0.040*\"efficient\" + 0.022*\"domain\" + 0.022*\"detection\" + 0.022*\"architecture\"')\n",
      "(4, '0.045*\"analysis\" + 0.045*\"protocol\" + 0.025*\"system\" + 0.025*\"program\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26, 1), (56, 1), (58, 1)]\n",
      "[(0, 0.050024964), (1, 0.55387896), (2, 0.29605168), (3, 0.05002095), (4, 0.05002346)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'Practical Bayesian Optimization of Machine Learning Algorithms'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Choosing the Right Text-Mining Techniques\n",
    "\n",
    "Table/Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "Blah\n",
    "\n",
    "Blah: \n",
    "\n",
    "* sdfsdfsdf\n",
    "* sdfsdfsdf\n",
    "\n",
    "👌👌👌\n",
    "\n",
    "The next notebook ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## What's Next?\n",
    "If you have decided that text-mining in Python is for you, then here are some more resources to study in your own time:\n",
    "\n",
    "* Go further in natural language processing, python...\n",
    "* Follow a more in-depth set of Jupyter notebooks [The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb).\n",
    "* Install Python using Anaconda on your computer: [Installing Anaconda on Windows](https://www.datacamp.com/community/tutorials/installing-anaconda-windows) [Installing Anaconda on Mac](https://www.datacamp.com/community/tutorials/installing-anaconda-mac-os-x).\n",
    "\n",
    "Even if you are not sure programming is for you, [Cambridge Digital Humanities](https://www.cdh.cam.ac.uk/) (CDH) has a number of resources to support your research. \n",
    "\n",
    "* CDH Learning - [training events/workshops](https://www.cdh.cam.ac.uk/learning/cdh-events) and mentoring programme\n",
    "* CDH Lab - email [lab@cdh.cam.ac.uk](mailto:lab@cdh.cam.ac.uk) for advice on your project, whether you are just getting started, somewhere in the middle, or thinking about the future\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
